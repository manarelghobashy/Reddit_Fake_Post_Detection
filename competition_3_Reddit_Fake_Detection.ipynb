{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6ra4pUjhTMKB",
        "xv8qUhdIXcHZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "CISC-873-DM-W23-A3 ‚ù§Ô∏è\n",
        "This Notebook is divided into two parts:\n",
        "1. Answring some questions about data mining subject. ü§ì\n",
        "2. A3 Reddit Fake Post Detection problem.\n",
        "\n",
        "Kaggle account name: Manar Elghobashy."
      ],
      "metadata": {
        "id": "aMDAPn_PKLrc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Answring some questions about data mining subjectü§ì:\n",
        "-----------------------------------------------------------------------\n",
        "#### Q1:What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
        "Character n-gram is a contiguous series of n characters from a given sample of text or speech, whereas word n-grams is is a contiguous series of n words from a given sample of text or speech, and the one who will suffer from the out-of-vocabulary issue is word n-gram\n",
        "* Character n-gram\n",
        "  * Character n-grams are found in text documents by representing the document as a sequence of characters.\n",
        "  * Character n-grams have proven to be of high quality for authorship attribution.\n",
        "  *Character n-grams are including whitespaces and punctuation.\n",
        "  * example, a character 4-gram model results in the following tokens: [It], [It_i], [t_is], [is], [is_a], [s_a_], [_a_s].\n",
        "* word n-gram\n",
        "  * Word n-grams are found in text documents by representing the document as a sequence of words.\n",
        "  * example, a character 1-gram model results in the following tokens: [hello], [like], [eat].\n",
        "  \n",
        "* The word is OOV. An Out-Of-Vocabulary (OOV) Word is a Linguistic Unit or a token that does not appear in training vocabulary or document.\n",
        "--------------------------------------------------------------------------------\n",
        "### Q2:What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
        "Both, Are the most important preprocessing techniques for text, but the main difference is that the stop word removal totally remove some predefined words that it knows from the sentence and have a list of these words, in other hand, stemming is about to reduce the word and return it to its root, like removing suffix and prefixes not removing the whole word, ex (playing -> play), etc, both are language dependant stop words in English not like in German and vice versa also the grammars in English not like in the German language.\n",
        "* stop words:\n",
        "  * Stop words are a set of commonly used words in a language. Examples of stop words in English are ‚Äúa‚Äù, ‚Äúthe‚Äù, ‚Äúis‚Äù, ‚Äúare‚Äù and etc.\n",
        "  * Stop words are commonly used in Text Mining and Natural Language Processing (NLP) to eliminate words that are so commonly used that they carry very little useful information.\n",
        "* stemming:\n",
        "  * Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. Examples of stemming in English are \"studies\" when we use the stemming it will be \"studi\".\n",
        "* the stop words and the stemming are language-dependent.\n",
        "--------------------------------------------------------------------------------\n",
        "### Q3: Is tokenization techniques language dependent? Why?\n",
        "* yes, tokenization is dependent on language. Each language can have various linguistic rules and exceptions. Languages such as English identify token boundaries via whitespace and punctuation, but other languages such as Chinese require a more complex segmenter to extract tokens from a stream of text that does not contain any whitespaces.\n",
        "--------------------------------------------------------------------------------\n",
        "### Q4: What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
        "* count vectorizer used to convert a text into a vector-based on the frequency (count) of each word that appears throughout the text, whereas tf-idf vectorizer it divides into two parts TF which is refer to term frequency where we count how many time that the word appears in the text and then dividing it by the total numbers of words in the whole text, where IDF is about taking the logarithm to the previous result to decrease the weight of a common word in the sentence or document, it wouldn't be feasiable and it would be np-complete problem,but you can select them by using some of search method techniques like (Grid search, random search).\n",
        "* The time it takes to create the count. Vectorizer is much lesser as compared to your hashing function or the tf-idf representation.\n",
        "* CountVectorizer: Counts the frequency of all words in our corpus, sorts them and grabs the most recurring features (using max_features hyperparameter). But these results are mostly biased and our model might loose out on some of the important less frequent features. These are all boolean values. Ex. SEO People used to take advantage of this.\n",
        "* TFIDFVectorizer: TFIDF is a statistical measure said to have fixed the issues with CountVectorizer in some way. It consists of 2 parts, TF (Term Frequency) multiplied with IDF (Inverse Document Frequency). The main intuition being some words that appear frequently in 1 document and less frequently in other documents could be considered as providing extra insight for that 1 document and could help our model learn from this additional piece of information. In short, common words are penalized. These are relative frequencies identified as floating point numbers.\n",
        "\n",
        "### Would it be feasible to use all possible n-grams? If not, how should you select them? \n",
        "* No, This will make it very difficult to assign likelihoods that capture the target of our analysis.\n",
        "* it will depand on the model and i will try different n to decide the best n. because If we consider a chunk size of n=2, our results include ‚ÄúThe reporters,‚Äù ‚Äúthe President,‚Äù ‚Äúthe United,‚Äù and ‚Äúthe room.‚Äù While not perfect, this model successfully identifies three of the relevant entities as candidates in a lightweight fashion.\n",
        "* On the other hand, a model based on the small n-gram window of 2 would fail to capture some of the nuance of the original text. For instance, if our sentence is from a text that references multiple heads of state, ‚Äúthe President‚Äù could be somewhat ambiguous. In order to capture the entirety of the phrase ‚Äúthe President of the United States,‚Äù\n",
        "-------------------------------------------------------------------------------\n",
        "\n"
      ],
      "metadata": {
        "id": "WrdzSkSEKYGG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Problem formulation:\n",
        "our problem here is about building a model to classify and detect which real news and fake news from just its titles, our inputs here are news titles (59151) observations for a training dataset and the output is (59151) label.\n",
        "\n",
        "\n",
        "### Data mining function:\n",
        "text preprocessing (stemming and lemmatizing) -> tokenization and vectorization each text -> building and training the model -> classification and prediction.\n",
        "### The challenges:\n",
        "We have a quite big data set so maybe will take some time to preprocess it and train it.\n",
        "\n",
        "Our input here is dirty text, each observation here has a lot of punctuation  and misspellings, and grammatical mistakes because each title here is typed by humans, so we need to choose a proper text cleaning technique by trying each one them and choose the one who give us best results.\n",
        "\n",
        "Even if we cleaned our input here, it's still string data and machine learning models can't deal with string data so we need to convert them into numeric data by technique called vectorization.\n",
        "\n",
        "#### The impact:\n",
        "Building this model will solve many social problems and prevent the spread of rumors on social media quickly.\n",
        "\n",
        "#### Experimental protocol:\n",
        "* Read training data.\n",
        "* Preprocessing:\n",
        "  * cleaning input data for both training set and test set using some regular expressions patterns and stemming or lemmatizing each text.\n",
        "* Data exploration\n",
        "* Data vectorization: There are two famous vectorization techniques count vectorizer and tf-idf vectorizer we will try each one of them and choose the one who will give us best results.\n",
        "* Building our piplines (has the vectorizer and machine learning model)\n",
        "* Build the search speace and search for the best hyperparameters combinations but trying many fits.\n",
        "* Test our model\n",
        "* Choose the best one\n",
        "--------------------------------------------------------------------------------\n"
      ],
      "metadata": {
        "id": "Br5MDtwfRIWe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## In this notebook:\n",
        "1. read and preprocess the data.\n",
        "2. build pipelines and models:\n",
        "  * XGB model\n",
        "  * Logistic Regression\n",
        "  * Random Forest."
      ],
      "metadata": {
        "id": "ip9a72ChSt26"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. read and preprocess the data"
      ],
      "metadata": {
        "id": "6ra4pUjhTMKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "import required data"
      ],
      "metadata": {
        "id": "3XDcpgxHTd4_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "6iARQ2GreKYb"
      },
      "outputs": [],
      "source": [
        "# important some required libraries.\n",
        "import re # for regular expressions.\n",
        "import pickle # to convert object to byte stream.\n",
        "import pandas as pd # # data processing, CSV file I/O.\n",
        "import numpy as np # linear algebra.\n",
        "pd.options.display.max_columns = None\n",
        "pd.options.display.max_rows = 100\n",
        "pd.options.display.max_colwidth = 100\n",
        "np.set_printoptions(threshold=2000)\n",
        "# import required libraries for building the model.\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import PredefinedSplit\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Convert a collection of raw documents to a matrix of TF-IDF features.\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from nltk.stem.snowball import SnowballStemmer # Natural Language Toolkit\n",
        "from nltk.tokenize import word_tokenize # NLTK Tokenizer Package.\n",
        "from nltk.corpus import stopwords #contains all the stop words.\n",
        "from nltk.stem import WordNetLemmatizer #lemmatization package.\n",
        "import nltk \n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tag import pos_tag"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read train/test data csv files."
      ],
      "metadata": {
        "id": "51UX--D2TjUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv files for train and test data.\n",
        "train_data=pd.read_csv(\"xy_train.csv\", index_col='id')\n",
        "test_data=pd.read_csv(\"x_test.csv\")\n",
        "sample_data=pd.read_csv(\"sample_submission.csv\")\n",
        "id_test=test_data['id']"
      ],
      "metadata": {
        "id": "pjHVPZ8fnDGB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "display some information of the data"
      ],
      "metadata": {
        "id": "nq09lze8T1Cu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display some information on the sample data to see how we will create the submission file.\n",
        "sample_data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I4MSd1gogtp",
        "outputId": "25af4d02-cd90-40a0-887c-e46f52f916ad"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 59151 entries, 0 to 59150\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   id      59151 non-null  int64  \n",
            " 1   label   59151 non-null  float64\n",
            "dtypes: float64(1), int64(1)\n",
            "memory usage: 924.4 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "3Nx912daooQ5",
        "outputId": "39f8163a-0b3c-4776-d5d6-bac3f7d4bdc9"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                                                                       text  \\\n",
              "id                                                                                                            \n",
              "265723  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
              "284269  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
              "207715  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
              "551106  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
              "8584    Obama to Nation: ËÅô\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
              "\n",
              "        label  \n",
              "id             \n",
              "265723      0  \n",
              "284269      0  \n",
              "207715      0  \n",
              "551106      0  \n",
              "8584        0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4d4c4d55-047b-45ad-a539-41cc493f494f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>265723</th>\n",
              "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>284269</th>\n",
              "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>207715</th>\n",
              "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>551106</th>\n",
              "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8584</th>\n",
              "      <td>Obama to Nation: ËÅô\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4d4c4d55-047b-45ad-a539-41cc493f494f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4d4c4d55-047b-45ad-a539-41cc493f494f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4d4c4d55-047b-45ad-a539-41cc493f494f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ycfuo_vWox-w",
        "outputId": "e410e977-1ed9-425c-f86d-afc694f59d37"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   id                                                            text\n",
              "0   0                                                      stargazer \n",
              "1   1                                                            yeah\n",
              "2   2      PD: Phoenix car thief gets instructions from YouTube video\n",
              "3   3  As Trump Accuses Iran, He Has One Problem: His Own Credibility\n",
              "4   4                                    \"Believers\" - Hezbollah 2011"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-230a78b9-e560-4592-ade4-fa9022a912d0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>stargazer</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>yeah</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>PD: Phoenix car thief gets instructions from YouTube video</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>As Trump Accuses Iran, He Has One Problem: His Own Credibility</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>\"Believers\" - Hezbollah 2011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-230a78b9-e560-4592-ade4-fa9022a912d0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-230a78b9-e560-4592-ade4-fa9022a912d0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-230a78b9-e560-4592-ade4-fa9022a912d0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "check if there is null in the data"
      ],
      "metadata": {
        "id": "RQaGkaJKT6gO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see if there is null in the train data.\n",
        "train_data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HR5B2_3o2I9",
        "outputId": "dec8ffd0-b327-4b25-ef8d-5a0f01012905"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "text     0\n",
              "label    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see if there is null in the test data.\n",
        "test_data.isnull().sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HtCgz0kvo9fx",
        "outputId": "ea9d62ae-52cb-458c-db5c-41da60c8be9e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id      0\n",
              "text    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " some preprocess on the label column"
      ],
      "metadata": {
        "id": "46JAGeI0UJzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see the unique values of the label column.\n",
        "train_data['label'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kF0MxjWpBlg",
        "outputId": "0c27a4e1-e037-4947-a429-ce74c7cb1bae"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see count of every label in the label column.\n",
        "train_data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQjghrjppJXM",
        "outputId": "58c54473-a552-4034-ead0-7c9fcab6cc4a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    32172\n",
              "1    27596\n",
              "2      232\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# drop label =2 as the label must be 0 or 1.\n",
        "train_data = train_data.drop(train_data[train_data.label == 2].index)\n",
        "train_data.label = train_data.label.astype('int8')"
      ],
      "metadata": {
        "id": "PcYquqScpTly"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['label'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JS4Y_29-rO-D",
        "outputId": "a13e983c-c8e0-40c1-9a98-8ead518448fb"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    32172\n",
              "1    27596\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, three functions are built:\n",
        "* one named \"clean_Regex\" is to remove punctuation, stop words, etc.\n",
        "* the second one is to apply lemmatizations and this function called \"lemmatize_clean_text\"\n",
        "* the last one, is for apply the stemming called \"stemming_clean_text\""
      ],
      "metadata": {
        "id": "EW4wc1WuUbYo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#download english stop words to remove the from the text.\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('tagsets')\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# build a function which take simple regex and apply them in the text.\n",
        "# this function will remove any html tags, single letter characters, and replace all whitespaces with sinhle whitespace\n",
        "def clean_Regex(text, for_embedding):\n",
        "    \"\"\"\n",
        "        - remove any html tags (< /br> often found)\n",
        "        - Keep only ASCII + European Chars and whitespace, no digits\n",
        "        - remove single letter chars\n",
        "        - convert all whitespaces (tabs etc.) to single wspace\n",
        "\n",
        "    \"\"\"\n",
        "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) \n",
        "    RE_TAGS = re.compile(r\"<.*?>\") \n",
        "    RE_ASCII = re.compile(r\"[^A-Za-z0-9]+\", re.IGNORECASE) \n",
        "    RE_SINGLECHAR = re.compile(r\"\\b^[^A-Za-z0-9]+\\b\", re.IGNORECASE) \n",
        "    if for_embedding:\n",
        "        # Keep punctuation\n",
        "        RE_ASCII = re.compile(r\"[^A-Za-z,.!? ]\", re.IGNORECASE) \n",
        "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-z,.!?]\\b\", re.IGNORECASE) \n",
        "\n",
        "    text = re.sub(RE_TAGS, \" \", text) \n",
        "    text = re.sub(RE_ASCII, \" \", text)\n",
        "    text = re.sub(RE_SINGLECHAR, \" \", text) \n",
        "    text = re.sub(RE_WSPACE, \" \", text) \n",
        "    word_tokens = word_tokenize(text) # a built-in function that will split the text into tokens. \n",
        "    \n",
        "    return word_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lf5T7dsRriy9",
        "outputId": "8af5a40e-fc68-4957-f051-a374067a6af2"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# function to determine the word tag\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V') or tag.startswith('M'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "# define a function that apply lemmatization on the text.\n",
        "# stemming identifies the original form of a word.\n",
        "# example: changing-->change, changes-->change.\n",
        "def lemmatize_clean_text(text ,for_embedding=False):\n",
        "\n",
        "    \"\"\" steps:\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and lemmatize\n",
        "    \"\"\"\n",
        "\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    word_tokens = clean_Regex(text, for_embedding)\n",
        "    pos_tags= pos_tag(word_tokens)\n",
        "    if for_embedding:\n",
        "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        # apply lemmatizing, lowering and punctuation / stopwords removal.\n",
        "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "        words_filtered = [lemmatizer.lemmatize(word,pos=get_wordnet_pos(pos)) for word, pos in pos_tags if word not in stop_words]\n",
        "\n",
        "    clean_text = \" \".join(words_filtered)\n",
        "    return clean_text"
      ],
      "metadata": {
        "id": "eDnsP-ejw1jy"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function that apply stemming on the text.\n",
        "# stemming identifies the common root form of a word by removing or replacing word suffixes.\n",
        "# example: changing-->chang, change-->chang.\n",
        "def stemming_clean_text(text ,for_embedding=False):\n",
        "    \"\"\" steps:\n",
        "        if not for embedding (but e.g. tdf-idf):\n",
        "        - all lowercase\n",
        "        - remove stopwords, punctuation and stemming\n",
        "    \"\"\"\n",
        "\n",
        "    stemmer = SnowballStemmer(\"english\")\n",
        "    # for_embedding by default false in the function.\n",
        "    word_tokens = clean_Regex(text,for_embedding=False)\n",
        "\n",
        "    if for_embedding:\n",
        "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
        "        words_filtered = word_tokens\n",
        "    else:\n",
        "        # apply stemming, lowering and punctuation / stopwords removal.\n",
        "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
        "        words_filtered = [stemmer.stem(word) for word in words_tokens_lower if word not in stop_words]\n",
        "\n",
        "    processed_text = \" \".join(words_filtered)\n",
        "    return processed_text"
      ],
      "metadata": {
        "id": "xJkGImub1nJV"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test lemmatization and stemming functions."
      ],
      "metadata": {
        "id": "SLciYukIVZnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stemming_clean_text(\"changing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "pQeaZxY-e-Mg",
        "outputId": "8d1d6cda-4a37-4792-c01d-e932af2b8bed"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'chang'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatize_clean_text(\"changing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "C26YlU4UfMKQ",
        "outputId": "d7b6f33c-810f-4e33-8797-bdb503167a42"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'change'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "apply lemmatization and stemming on test in the train/test data"
      ],
      "metadata": {
        "id": "tRZD9ovXVgEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply the lammatization/stemming on the training and testing data.\n",
        "data_lemmatized = train_data[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## clean and lemmatiz training set\n",
        "data_stemmed = train_data[\"text\"].map(lambda x: stemming_clean_text(x, for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and stemming training set\n",
        "test_lemmatized = test_data[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set\n",
        "test_stemmed = test_data[\"text\"].map(lambda x: stemming_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and stemming testing data"
      ],
      "metadata": {
        "id": "2ctrpfr4jiZF"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "see word frequencies by lemmatization and stemming."
      ],
      "metadata": {
        "id": "qvukUKzCVrFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_lemmatized = pd.Series(\" \".join(data_lemmatized).split()).value_counts()\n",
        "word_freq_lemmatized[1:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHL5QOEMkOVh",
        "outputId": "cf5be67c-855d-4a3b-97ab-f701641a8f24"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "I           8936\n",
              "A           5634\n",
              "This        4636\n",
              "0           4307\n",
              "To          3506\n",
              "year        3394\n",
              "get         2991\n",
              "make        2856\n",
              "like        2686\n",
              "Trump       2504\n",
              "It          2503\n",
              "look        2453\n",
              "one         2433\n",
              "In          2326\n",
              "say         2319\n",
              "find        2167\n",
              "take        2132\n",
              "Of          2108\n",
              "My          2039\n",
              "2           1987\n",
              "old         1938\n",
              "You         1925\n",
              "use         1894\n",
              "He          1874\n",
              "first       1762\n",
              "go          1736\n",
              "1           1697\n",
              "people      1668\n",
              "new         1611\n",
              "man         1567\n",
              "time        1545\n",
              "For         1484\n",
              "Is          1479\n",
              "They        1466\n",
              "poster      1446\n",
              "And         1434\n",
              "New         1414\n",
              "see         1411\n",
              "colorize    1361\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_stemmed = pd.Series(\" \".join(data_stemmed).split()).value_counts()\n",
        "word_freq_stemmed[1:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sB8tnZ-5kbed",
        "outputId": "f1d78369-8755-4aba-e554-493d44387554"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "year        4125\n",
              "one         3285\n",
              "like        3128\n",
              "new         2998\n",
              "look        2847\n",
              "color       2737\n",
              "man         2728\n",
              "get         2602\n",
              "trump       2578\n",
              "say         2347\n",
              "peopl       2316\n",
              "use         2307\n",
              "first       2248\n",
              "make        2227\n",
              "old         2226\n",
              "time        2027\n",
              "poster      2000\n",
              "found       1999\n",
              "2           1986\n",
              "day         1934\n",
              "war         1858\n",
              "1           1697\n",
              "post        1648\n",
              "world       1570\n",
              "work        1531\n",
              "show        1513\n",
              "american    1504\n",
              "us          1504\n",
              "take        1491\n",
              "life        1481\n",
              "psbattl     1470\n",
              "help        1442\n",
              "go          1418\n",
              "state       1409\n",
              "back        1369\n",
              "two         1364\n",
              "school      1345\n",
              "see         1329\n",
              "photo       1324\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word_freq_stemmed[-10:].reset_index(name=\"freq\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "2yAvDKxhklP9",
        "outputId": "49b0a275-1880-4a18-9345-912469089bbf"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           index  freq\n",
              "0          darli     1\n",
              "1         cvtwtq     1\n",
              "2     cornerston     1\n",
              "3           soot     1\n",
              "4         codina     1\n",
              "5        sebasti     1\n",
              "6      nearsight     1\n",
              "7  poaq69oe7ti31     1\n",
              "8      1741x2604     1\n",
              "9           110k     1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a49b5bc2-5183-4a5f-a211-14b36bb5916e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>index</th>\n",
              "      <th>freq</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>darli</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>cvtwtq</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>cornerston</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>soot</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>codina</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>sebasti</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>nearsight</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>poaq69oe7ti31</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>1741x2604</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>110k</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a49b5bc2-5183-4a5f-a211-14b36bb5916e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-a49b5bc2-5183-4a5f-a211-14b36bb5916e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-a49b5bc2-5183-4a5f-a211-14b36bb5916e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "display labels values in the training data"
      ],
      "metadata": {
        "id": "VQjafCm-Vz06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a visualzation of the label values and thier count in the train data file.\n",
        "train_data[\"label\"].value_counts(normalize=True).plot(kind='bar',title='Balanced dataset')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 465
        },
        "id": "pgGMgNbekmW-",
        "outputId": "b509e019-b907-4fe2-ba63-24d1f78d39c7"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: title={'center': 'Balanced dataset'}>"
            ]
          },
          "metadata": {},
          "execution_count": 50
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGuCAYAAABC7AYqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAig0lEQVR4nO3df1RUdf7H8RegzKAm/qBAka/4a1UyxWAxrPxRuFia2Smztg2aks6ejrt6WGuz3aTsx1iZ4bqmZaJluZpm1nHLH7F5znZioyDTNDtm/uoHIFZgWGBwv394nHYWMAaVt8Dzcc49OXc+985nbk08vXNnCHIcxxEAAICRYOsJAACA1o0YAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBmrHY2Fjddttt1tM4bcuXL1dQUJD279/fqO1vu+02xcbGntE5AWg6xAjQBE7+sP3v5YILLtDo0aP15ptvWk+vVXv00Ue1fv1662lIknbt2qUHHnig0VEGNFfECNCEZs+erRUrVuiFF17QPffco8OHD+vqq6/Whg0brKfWap1rMfLggw8SI2h12lhPAGhNrrrqKiUmJvpu33HHHYqMjNQ//vEPjR8/3nBmAGCHMyOAoU6dOiksLExt2vj/vWDu3LkaPny4unbtqrCwMCUkJGjt2rW/uL9vvvlGM2bM0EUXXaQOHTqoY8eOuuqqq/TRRx/5jdu6dauCgoL08ssv65FHHlGPHj3kdrt15ZVX6rPPPqu13/fee09XX321OnfurPbt22vw4MGaP3++35jdu3frhhtuUJcuXeR2u5WYmKjXX3+91r527typK664QmFhYerRo4cefvhh1dTUNORwSZLWr1+vQYMGye12a9CgQXr11VfrHNeQYxgUFKSKigo9//zzvrfPTl6Dc+DAAd11113q37+/wsLC1LVrV02aNKnWWYvjx4/rwQcfVL9+/eR2u9W1a1dddtll2rJlS0DHZ/ny5Zo0aZIkafTo0b75bN26tcHHBmiuODMCNKGysjKVlpbKcRyVlJRowYIF+v777/W73/3Ob9z8+fM1YcIE3XLLLaqqqtKqVas0adIkbdiwQePGjat3/59//rnWr1+vSZMmqVevXiouLtYzzzyjkSNHateuXerevbvf+Dlz5ig4OFgzZsxQWVmZHn/8cd1yyy167733fGO2bNmi8ePHq1u3bpo2bZqioqL0ySefaMOGDZo2bZqkE4Fx6aWXKjo6Wvfee6/at2+vl19+WRMnTtQrr7yi6667TpJUVFSk0aNH66effvKNe/bZZxUWFtag47d582Zdf/31iouLk9fr1ZEjR+TxeNSjR49aYxtyDFesWKEpU6YoKSlJd955pySpT58+kqT3339f7777rm666Sb16NFD+/fv16JFizRq1Cjt2rVL7dq1kyQ98MAD8nq9vv2Ul5frgw8+UGFhocaMGdPg4zNixAj98Y9/1N/+9jfdd999GjhwoCT5/gm0aA6As27ZsmWOpFqLy+Vyli9fXmv8sWPH/G5XVVU5gwYNcq644gq/9T179nTS09N9t3/88Uenurrab8y+ffscl8vlzJ4927fu7bffdiQ5AwcOdCorK33r58+f70hyduzY4TiO4/z0009Or169nJ49ezrffvut335ramp8f77yyiudiy66yPnxxx/97h8+fLjTr18/37rp06c7kpz33nvPt66kpMQJDw93JDn79u2rdSz+W3x8vNOtWzfnu+++863bvHmzI8np2bOn39iGHsP27dv7HcP6tnccx8nLy3MkOS+88IJv3ZAhQ5xx48adct4NPT5r1qxxJDlvv/32KfcHtDS8TQM0oYULF2rLli3asmWLXnzxRY0ePVpTpkzRunXr/Mb995mCb7/9VmVlZbr88stVWFh4yv27XC4FB594WVdXV+vIkSPq0KGD+vfvX+e2Ho9HoaGhvtuXX365pBNnWCTpww8/1L59+zR9+nR16tTJb9ugoCBJJ94a+te//qUbb7xRR48eVWlpqUpLS3XkyBGlpqZqz549+vLLLyVJb7zxhi655BIlJSX59nP++efrlltuOeXzkqSvv/5a27ZtU3p6usLDw33rx4wZo7i4uFrjG3sM69r++PHjOnLkiPr27atOnTr57aNTp07auXOn9uzZU+d+Ajk+QGvF2zRAE0pKSvK7gPXmm2/W0KFDNXXqVI0fP94XBhs2bNDDDz+sbdu2qbKy0jf+ZADUp6amRvPnz9fTTz+tffv2qbq62ndf165da43/v//7P7/bnTt3lnTih7ck7d27V5I0aNCgeh/zs88+k+M4uv/++3X//ffXOaakpETR0dE6cOCAhg0bVuv+/v37n/J5SSeu4ZCkfv361bn9/0ZGY4/hST/88IO8Xq+WLVumL7/8Uo7j+O4rKyvz/Xn27Nm69tpr9atf/UqDBg3S2LFjdeutt2rw4MGSAjs+QGtFjACGgoODNXr0aM2fP1979uzRhRdeqH//+9+aMGGCRowYoaefflrdunVT27ZttWzZMq1cufKU+3v00Ud1//336/bbb9dDDz2kLl26KDg4WNOnT6/zItGQkJA69/PfP3h/ycn9zpgxQ6mpqXWO6du3b4P3dyaczjE86Q9/+IOWLVum6dOnKzk5WeHh4QoKCtJNN93kdyxHjBihvXv36rXXXtPmzZv13HPP6amnntLixYs1ZcqUc/L4AOcaYgQw9tNPP0mSvv/+e0nSK6+8IrfbrU2bNsnlcvnGLVu27Bf3tXbtWo0ePVpLly71W//dd98pIiIi4LmdvJjz448/VkpKSp1jevfuLUlq27ZtvWNO6tmzZ51vZ3z66ae/OJeePXtKUoO2D+QY1nemZO3atUpPT9eTTz7pW/fjjz/qu+++qzW2S5cu8ng88ng8+v777zVixAg98MADmjJlSkDHp6FnbYCWhmtGAEPHjx/X5s2bFRoa6vvUREhIiIKCgvzeYtm/f3+DvpgrJCSk1lmNNWvWNPqahIsvvli9evVSdnZ2rR/CJx/nggsu0KhRo/TMM8/o66+/rrWPw4cP+/589dVX6z//+Y/y8/P97n/ppZd+cS7dunVTfHy8nn/+eb+3SbZs2aJdu3b5jQ3kGLZv377OwKjrWC5YsMBvn5J05MgRv9sdOnRQ3759fW8NBXJ82rdvL0l1zgdoyTgzAjShN998U7t375Z04jqBlStXas+ePbr33nvVsWNHSdK4ceM0b948jR07Vr/97W9VUlKihQsXqm/fvtq+ffsp9z9+/HjNnj1bHo9Hw4cP144dO/TSSy/5/nYeqODgYC1atEjXXHON4uPj5fF41K1bN+3evVs7d+7Upk2bJJ24MPeyyy7TRRddpIyMDPXu3VvFxcXKy8vTF1984fuek3vuuUcrVqzQ2LFjNW3aNN9He3v27PmLz02SvF6vxo0bp8suu0y33367vvnmGy1YsEAXXnih78xSoMcwISFBb731lubNm6fu3burV69eGjZsmMaPH68VK1YoPDxccXFxysvL01tvvVXr2pu4uDiNGjVKCQkJ6tKliz744AOtXbtWU6dO9Y1p6PGJj49XSEiIHnvsMZWVlcnlcumKK67QBRdc0Kh/f0CzYfhJHqDVqOujvW6324mPj3cWLVrk9zFZx3GcpUuXOv369XNcLpczYMAAZ9myZU5WVpbzvy/Zuj7a+6c//cnp1q2bExYW5lx66aVOXl6eM3LkSGfkyJG+cSc/2rtmzRq//e3bt8+R5Cxbtsxv/TvvvOOMGTPGOe+885z27ds7gwcPdhYsWOA3Zu/evU5aWpoTFRXltG3b1omOjnbGjx/vrF271m/c9u3bnZEjRzput9uJjo52HnroIWfp0qUN+miv4zjOK6+84gwcONBxuVxOXFycs27dOic9Pb3WR3sbegx3797tjBgxwgkLC3Mk+Y7nt99+63g8HiciIsLp0KGDk5qa6uzevbvWMX/44YedpKQkp1OnTk5YWJgzYMAA55FHHnGqqqoadXyWLFni9O7d2wkJCeFjvmg1ghwngCvVAAAAzjCuGQEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCqWXzpWU1Njb766iudd955fF0yAADNhOM4Onr0qLp37+77jeJ1aRYx8tVXXykmJsZ6GgAAoBEOHTqkHj161Ht/s4iR8847T9KJJ3PyK7MBAMC5rby8XDExMb6f4/VpFjFy8q2Zjh07EiMAADQzv3SJBRewAgAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAw1cZ6Aji12Hv/aT0FNKH9c8ZZTwEAmhxnRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmCJGAACAKWIEAACYIkYAAIApYgQAAJgiRgAAgCliBAAAmGpUjCxcuFCxsbFyu90aNmyY8vPz6x27fPlyBQUF+S1ut7vREwYAAC1LwDGyevVqZWZmKisrS4WFhRoyZIhSU1NVUlJS7zYdO3bU119/7VsOHDhwWpMGAAAtR8AxMm/ePGVkZMjj8SguLk6LFy9Wu3btlJOTU+82QUFBioqK8i2RkZGnNWkAANByBBQjVVVVKigoUEpKys87CA5WSkqK8vLy6t3u+++/V8+ePRUTE6Nrr71WO3fuPOXjVFZWqry83G8BAAAtU0AxUlpaqurq6lpnNiIjI1VUVFTnNv3791dOTo5ee+01vfjii6qpqdHw4cP1xRdf1Ps4Xq9X4eHhviUmJiaQaQIAgGbkrH+aJjk5WWlpaYqPj9fIkSO1bt06nX/++XrmmWfq3WbmzJkqKyvzLYcOHTrb0wQAAEbaBDI4IiJCISEhKi4u9ltfXFysqKioBu2jbdu2Gjp0qD777LN6x7hcLrlcrkCmBgAAmqmAzoyEhoYqISFBubm5vnU1NTXKzc1VcnJyg/ZRXV2tHTt2qFu3boHNFAAAtEgBnRmRpMzMTKWnpysxMVFJSUnKzs5WRUWFPB6PJCktLU3R0dHyer2SpNmzZ+uSSy5R37599d133+mJJ57QgQMHNGXKlDP7TAAAQLMUcIxMnjxZhw8f1qxZs1RUVKT4+Hht3LjRd1HrwYMHFRz88wmXb7/9VhkZGSoqKlLnzp2VkJCgd999V3FxcWfuWQAAgGYryHEcx3oSv6S8vFzh4eEqKytTx44drafTpGLv/af1FNCE9s8ZZz0FADhjGvrzm99NAwAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwFQb6wkAQGsVe+8/raeAJrR/zjjrKZyzODMCAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAFDECAABMESMAAMAUMQIAAEwRIwAAwBQxAgAATBEjAADAVKNiZOHChYqNjZXb7dawYcOUn5/foO1WrVqloKAgTZw4sTEPCwAAWqCAY2T16tXKzMxUVlaWCgsLNWTIEKWmpqqkpOSU2+3fv18zZszQ5Zdf3ujJAgCAlifgGJk3b54yMjLk8XgUFxenxYsXq127dsrJyal3m+rqat1yyy168MEH1bt379OaMAAAaFkCipGqqioVFBQoJSXl5x0EByslJUV5eXn1bjd79mxdcMEFuuOOOxr0OJWVlSovL/dbAABAyxRQjJSWlqq6ulqRkZF+6yMjI1VUVFTnNu+8846WLl2qJUuWNPhxvF6vwsPDfUtMTEwg0wQAAM3IWf00zdGjR3XrrbdqyZIlioiIaPB2M2fOVFlZmW85dOjQWZwlAACw1CaQwREREQoJCVFxcbHf+uLiYkVFRdUav3fvXu3fv1/XXHONb11NTc2JB27TRp9++qn69OlTazuXyyWXyxXI1AAAQDMV0JmR0NBQJSQkKDc317eupqZGubm5Sk5OrjV+wIAB2rFjh7Zt2+ZbJkyYoNGjR2vbtm28/QIAAAI7MyJJmZmZSk9PV2JiopKSkpSdna2Kigp5PB5JUlpamqKjo+X1euV2uzVo0CC/7Tt16iRJtdYDAIDWKeAYmTx5sg4fPqxZs2apqKhI8fHx2rhxo++i1oMHDyo4mC92BQAADRNwjEjS1KlTNXXq1Drv27p16ym3Xb58eWMeEgAAtFCcwgAAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCqUTGycOFCxcbGyu12a9iwYcrPz6937Lp165SYmKhOnTqpffv2io+P14oVKxo9YQAA0LIEHCOrV69WZmamsrKyVFhYqCFDhig1NVUlJSV1ju/SpYv+8pe/KC8vT9u3b5fH45HH49GmTZtOe/IAAKD5CzhG5s2bp4yMDHk8HsXFxWnx4sVq166dcnJy6hw/atQoXXfddRo4cKD69OmjadOmafDgwXrnnXdOe/IAAKD5CyhGqqqqVFBQoJSUlJ93EByslJQU5eXl/eL2juMoNzdXn376qUaMGFHvuMrKSpWXl/stAACgZQooRkpLS1VdXa3IyEi/9ZGRkSoqKqp3u7KyMnXo0EGhoaEaN26cFixYoDFjxtQ73uv1Kjw83LfExMQEMk0AANCMNMmnac477zxt27ZN77//vh555BFlZmZq69at9Y6fOXOmysrKfMuhQ4eaYpoAAMBAm0AGR0REKCQkRMXFxX7ri4uLFRUVVe92wcHB6tu3ryQpPj5en3zyibxer0aNGlXneJfLJZfLFcjUAABAMxXQmZHQ0FAlJCQoNzfXt66mpka5ublKTk5u8H5qampUWVkZyEMDAIAWKqAzI5KUmZmp9PR0JSYmKikpSdnZ2aqoqJDH45EkpaWlKTo6Wl6vV9KJ6z8SExPVp08fVVZW6o033tCKFSu0aNGiM/tMAABAsxRwjEyePFmHDx/WrFmzVFRUpPj4eG3cuNF3UevBgwcVHPzzCZeKigrddddd+uKLLxQWFqYBAwboxRdf1OTJk8/cswAAAM1WkOM4jvUkfkl5ebnCw8NVVlamjh07Wk+nScXe+0/rKaAJ7Z8zznoKaEK8vluX1vj6bujPb343DQAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMEWMAAAAU8QIAAAwRYwAAABTxAgAADBFjAAAAFPECAAAMNWoGFm4cKFiY2Pldrs1bNgw5efn1zt2yZIluvzyy9W5c2d17txZKSkppxwPAABal4BjZPXq1crMzFRWVpYKCws1ZMgQpaamqqSkpM7xW7du1c0336y3335beXl5iomJ0W9+8xt9+eWXpz15AADQ/AUcI/PmzVNGRoY8Ho/i4uK0ePFitWvXTjk5OXWOf+mll3TXXXcpPj5eAwYM0HPPPaeamhrl5uae9uQBAEDzF1CMVFVVqaCgQCkpKT/vIDhYKSkpysvLa9A+jh07puPHj6tLly71jqmsrFR5ebnfAgAAWqaAYqS0tFTV1dWKjIz0Wx8ZGamioqIG7ePPf/6zunfv7hc0/8vr9So8PNy3xMTEBDJNAADQjDTpp2nmzJmjVatW6dVXX5Xb7a533MyZM1VWVuZbDh061ISzBAAATalNIIMjIiIUEhKi4uJiv/XFxcWKioo65bZz587VnDlz9NZbb2nw4MGnHOtyueRyuQKZGgAAaKYCOjMSGhqqhIQEv4tPT16MmpycXO92jz/+uB566CFt3LhRiYmJjZ8tAABocQI6MyJJmZmZSk9PV2JiopKSkpSdna2Kigp5PB5JUlpamqKjo+X1eiVJjz32mGbNmqWVK1cqNjbWd21Jhw4d1KFDhzP4VAAAQHMUcIxMnjxZhw8f1qxZs1RUVKT4+Hht3LjRd1HrwYMHFRz88wmXRYsWqaqqSjfccIPffrKysvTAAw+c3uwBAECzF3CMSNLUqVM1derUOu/bunWr3+39+/c35iEAAEArwe+mAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYKpRMbJw4ULFxsbK7XZr2LBhys/Pr3fszp07df311ys2NlZBQUHKzs5u7FwBAEALFHCMrF69WpmZmcrKylJhYaGGDBmi1NRUlZSU1Dn+2LFj6t27t+bMmaOoqKjTnjAAAGhZAo6RefPmKSMjQx6PR3FxcVq8eLHatWunnJycOsf/+te/1hNPPKGbbrpJLpfrtCcMAABaloBipKqqSgUFBUpJSfl5B8HBSklJUV5e3hmbVGVlpcrLy/0WAADQMgUUI6WlpaqurlZkZKTf+sjISBUVFZ2xSXm9XoWHh/uWmJiYM7ZvAABwbjknP00zc+ZMlZWV+ZZDhw5ZTwkAAJwlbQIZHBERoZCQEBUXF/utLy4uPqMXp7pcLq4vAQCglQjozEhoaKgSEhKUm5vrW1dTU6Pc3FwlJyef8ckBAICWL6AzI5KUmZmp9PR0JSYmKikpSdnZ2aqoqJDH45EkpaWlKTo6Wl6vV9KJi1537drl+/OXX36pbdu2qUOHDurbt+8ZfCoAAKA5CjhGJk+erMOHD2vWrFkqKipSfHy8Nm7c6Luo9eDBgwoO/vmEy1dffaWhQ4f6bs+dO1dz587VyJEjtXXr1tN/BgAAoFkLOEYkaerUqZo6dWqd9/1vYMTGxspxnMY8DAAAaAXOyU/TAACA1oMYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKYaFSMLFy5UbGys3G63hg0bpvz8/FOOX7NmjQYMGCC3262LLrpIb7zxRqMmCwAAWp6AY2T16tXKzMxUVlaWCgsLNWTIEKWmpqqkpKTO8e+++65uvvlm3XHHHfrwww81ceJETZw4UR9//PFpTx4AADR/AcfIvHnzlJGRIY/Ho7i4OC1evFjt2rVTTk5OnePnz5+vsWPH6u6779bAgQP10EMP6eKLL9bf//730548AABo/toEMriqqkoFBQWaOXOmb11wcLBSUlKUl5dX5zZ5eXnKzMz0W5eamqr169fX+ziVlZWqrKz03S4rK5MklZeXBzLdFqGm8pj1FNCEWuN/460Zr+/WpTW+vk8+Z8dxTjkuoBgpLS1VdXW1IiMj/dZHRkZq9+7ddW5TVFRU5/iioqJ6H8fr9erBBx+stT4mJiaQ6QLNTni29QwAnC2t+fV99OhRhYeH13t/QDHSVGbOnOl3NqWmpkbffPONunbtqqCgIMOZoSmUl5crJiZGhw4dUseOHa2nA+AM4vXdujiOo6NHj6p79+6nHBdQjERERCgkJETFxcV+64uLixUVFVXnNlFRUQGNlySXyyWXy+W3rlOnToFMFS1Ax44d+Z8V0ELx+m49TnVG5KSALmANDQ1VQkKCcnNzfetqamqUm5ur5OTkOrdJTk72Gy9JW7ZsqXc8AABoXQJ+myYzM1Pp6elKTExUUlKSsrOzVVFRIY/HI0lKS0tTdHS0vF6vJGnatGkaOXKknnzySY0bN06rVq3SBx98oGefffbMPhMAANAsBRwjkydP1uHDhzVr1iwVFRUpPj5eGzdu9F2kevDgQQUH/3zCZfjw4Vq5cqX++te/6r777lO/fv20fv16DRo06Mw9C7QoLpdLWVlZtd6qA9D88fpGXYKcX/q8DQAAwFnE76YBAACmiBEAAGCKGAEAAKaIEQAAYIoYAQAAps7Jr4NH61JaWqqcnBzl5eX5fmdRVFSUhg8frttuu03nn3++8QwBAGcTH+2Fqffff1+pqalq166dUlJSfN9XU1xcrNzcXB07dkybNm1SYmKi8UwBAGcLMQJTl1xyiYYMGaLFixfX+iWIjuPo97//vbZv3668vDyjGQI4Ww4dOqSsrCzl5ORYTwXGiBGYCgsL04cffqgBAwbUef/u3bs1dOhQ/fDDD008MwBn20cffaSLL75Y1dXV1lOBMa4ZgamoqCjl5+fXGyP5+fm+t24ANC+vv/76Ke///PPPm2gmONcRIzA1Y8YM3XnnnSooKNCVV15Z65qRJUuWaO7cucazBNAYEydOVFBQkE51Av5/355F68TbNDC3evVqPfXUUyooKPCdrg0JCVFCQoIyMzN14403Gs8QQGNER0fr6aef1rXXXlvn/du2bVNCQgJv04AYwbnj+PHjKi0tlSRFRESobdu2xjMCcDomTJig+Ph4zZ49u877P/roIw0dOlQ1NTVNPDOca3ibBueMtm3bqlu3btbTAHCG3H333aqoqKj3/r59++rtt99uwhnhXMWZEQAAYIqvgwcAAKaIEQAAYIoYAQAApogRAABgihgBAACmiBEAAGCKGAEAAKb+H6Qwnvp9pDIoAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "split the lemmatize and stemmed data into train and validation"
      ],
      "metadata": {
        "id": "u2GRpIeIWaDm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#split data_lemmatized feature\n",
        "\n",
        "# split the original training set to a train and a validation set becuase we will use them in search method\n",
        "\n",
        "X_train_1 , X_val_1 , Y_train_1, Y_val_1 = train_test_split(data_lemmatized,train_data['label'],stratify=train_data['label'], random_state=42, test_size=0.25, shuffle=True)\n",
        "\n",
        "\n",
        "split_index_lemmatized = [-1 if x in X_train_1.index else 0 for x in data_lemmatized.index]\n",
        "\n",
        "print(X_train_1.shape)\n",
        "print(X_val_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tl2Ib7okrM8",
        "outputId": "45ffc587-e75e-431f-8a37-727eb9c798a7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44826,)\n",
            "(14942,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#split data_stemmed feature\n",
        "\n",
        "#Split the original training set to a train and a validation set becuase we will use them in search method\n",
        "\n",
        "X_train_2, X_val_2 ,Y_train_2, Y_val_2 = train_test_split(data_stemmed,train_data['label'],stratify=train_data['label'], random_state=42, test_size=0.25, shuffle=True)\n",
        "\n",
        "\n",
        "split_index_stemmed = [-1 if x in X_train_2.index else 0 for x in data_stemmed.index]\n",
        "\n",
        "\n",
        "print(X_train_2.shape)\n",
        "print(X_val_2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLseE890k15M",
        "outputId": "5b57f0dc-a13f-48c7-f4a1-6fa3d448a66e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(44826,)\n",
            "(14942,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. build pipelines and models:\n"
      ],
      "metadata": {
        "id": "xv8qUhdIXcHZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 logistic regression:\n",
        "* two trails are applied using logistic regression one using lemmatized data and one using stemmed data."
      ],
      "metadata": {
        "id": "ADZ1TiNLYExw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.1 logistic regression using stemmed data and validation set:\n",
        "in this trial the hyperparameter space iclude:\n",
        "* ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)]\n",
        "* max_df\": np.arange(0.2, 1.0)\n",
        "* min_df\": np.arange(5, 100)\n",
        "* analyzer\": ['word','char','char_wb']\n",
        "* strip_accents\":[None,'ascii','unicode']\n",
        "* smooth_idf':[False,True]\n",
        "* sublinear_tf\":[True,False] \n",
        "\n",
        "and a random search is used using validation set\n"
      ],
      "metadata": {
        "id": "bRyLOfbcZePN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature creation and modelling in a single function\n",
        "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
        "\n",
        "ps_1 = PredefinedSplit(split_index_stemmed)\n",
        "\n",
        "\n",
        "# define parameter space to test\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    \"tfidf__analyzer\": ['word','char','char_wb'],\n",
        "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
        "    'tfidf__smooth_idf':[False,True],\n",
        "    \"tfidf__sublinear_tf\":[True,False]\n",
        "}\n",
        "\n",
        "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
        "\n",
        "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=ps_1, scoring=\"roc_auc\", n_iter=50)\n",
        "pipe_lg_clf.fit(data_stemmed, train_data['label'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "LYAkeeUIlZDf",
        "outputId": "cebbcfb9-e6b5-4525-ba30-3721fda2b92e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                             ('lg',\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={'tfidf__analyzer': ['word', 'char',\n",
              "                                                            'char_wb'],\n",
              "                                        'tfidf__max_df': array([0.2]),\n",
              "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        'tfidf__smooth_idf': [False, True],\n",
              "                                        'tfidf__strip_accents': [None, 'ascii',\n",
              "                                                                 'unicode'],\n",
              "                                        'tfidf__sublinear_tf': [True, False]},\n",
              "                   scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;lg&#x27;,\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={&#x27;tfidf__analyzer&#x27;: [&#x27;word&#x27;, &#x27;char&#x27;,\n",
              "                                                            &#x27;char_wb&#x27;],\n",
              "                                        &#x27;tfidf__max_df&#x27;: array([0.2]),\n",
              "                                        &#x27;tfidf__min_df&#x27;: array([ 5,  6,  7,  8,  9, 10, 11...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;lg&#x27;,\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={&#x27;tfidf__analyzer&#x27;: [&#x27;word&#x27;, &#x27;char&#x27;,\n",
              "                                                            &#x27;char_wb&#x27;],\n",
              "                                        &#x27;tfidf__max_df&#x27;: array([0.2]),\n",
              "                                        &#x27;tfidf__min_df&#x27;: array([ 5,  6,  7,  8,  9, 10, 11...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;lg&#x27;,\n",
              "                 LogisticRegression(max_iter=10000, n_jobs=-1,\n",
              "                                    random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
        "print('best score {}'.format(pipe_lg_clf.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-N7c6BFAog8V",
        "outputId": "dc1b5c7b-c27e-4461-e7ea-f664916cef8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8866824259235542\n",
            "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a submission for the logreg using different search methods.\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = id_test\n",
        "\n",
        "submission['label'] = pipe_lg_clf.predict_proba(test_stemmed)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_lg_rand2.csv', index=False)"
      ],
      "metadata": {
        "id": "EIkiz5dfpK5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has hyperparaneters:\n",
        "* ngram_range\": [ (1,5)]\n",
        "* max_df\": 0.2\n",
        "* min_df\": 5\n",
        "* analyzer\": ['word ']\n",
        "* strip_accents\":['unicode']\n",
        "* smooth_idf':[False]\n",
        "* sublinear_tf\":[True]\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of word, char, and char_wb and the result was using analyzer word so here **word-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.88668\n",
        " * model accuracy in kaggle public: 0.81474\n",
        "--------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "auf1n6HAaYCH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.1.2 logistic regression using lemmatized data and validation set:\n",
        "in this trial the hyperparameter space iclude:\n",
        "* ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)]\n",
        "* max_df\": np.arange(0.2, 1.0)\n",
        "* min_df\": np.arange(5, 100)\n",
        "* analyzer\": ['char']\n",
        "* strip_accents\":[None,'ascii','unicode']\n",
        "* smooth_idf':[False,True]\n",
        "* sublinear_tf\":[True,False] \n",
        "\n",
        "and a random search is used using validation set\n",
        "here in the hyperparamter space the analyzer is set to 'char' so that **character-level vectorizer** is covered."
      ],
      "metadata": {
        "id": "kwpiDMvHb1i3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature creation and modelling in a single function\n",
        "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
        "\n",
        "ps_1 = PredefinedSplit(split_index_stemmed)\n",
        "\n",
        "\n",
        "# define parameter space to test\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    \"tfidf__analyzer\": ['char'],\n",
        "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
        "    'tfidf__smooth_idf':[False,True],\n",
        "    \"tfidf__sublinear_tf\":[True,False]\n",
        "}\n",
        "\n",
        "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
        "\n",
        "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=ps_1, scoring=\"roc_auc\", n_iter=50)\n",
        "pipe_lg_clf.fit(data_lemmatized, train_data['label'])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 152
        },
        "id": "XluIRt431Whe",
        "outputId": "ad6e9f02-2d13-4ac9-81fa-d858b7e01fc7"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                             ('lg',\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={'tfidf__analyzer': ['char'],\n",
              "                                        'tfidf__max_df': array([0.2]),\n",
              "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 1...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        'tfidf__smooth_idf': [False, True],\n",
              "                                        'tfidf__strip_accents': [None, 'ascii',\n",
              "                                                                 'unicode'],\n",
              "                                        'tfidf__sublinear_tf': [True, False]},\n",
              "                   scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;lg&#x27;,\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={&#x27;tfidf__analyzer&#x27;: [&#x27;char&#x27;],\n",
              "                                        &#x27;tfidf__max_df&#x27;: array([0.2]),\n",
              "                                        &#x27;tfidf__min_df&#x27;: array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 1...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;lg&#x27;,\n",
              "                                              LogisticRegression(max_iter=10000,\n",
              "                                                                 n_jobs=-1,\n",
              "                                                                 random_state=42))]),\n",
              "                   n_iter=50, n_jobs=-1,\n",
              "                   param_distributions={&#x27;tfidf__analyzer&#x27;: [&#x27;char&#x27;],\n",
              "                                        &#x27;tfidf__max_df&#x27;: array([0.2]),\n",
              "                                        &#x27;tfidf__min_df&#x27;: array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 1...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;lg&#x27;,\n",
              "                 LogisticRegression(max_iter=10000, n_jobs=-1,\n",
              "                                    random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=10000, n_jobs=-1, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
        "print('best score {}'.format(pipe_lg_clf.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZObs2nUx1v4o",
        "outputId": "58fc8049-444e-4555-d3a2-e140fd086ce1"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.9068108280220225\n",
            "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 56, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# make a submission for the logreg using different search methods.\n",
        "submission = pd.DataFrame()\n",
        "\n",
        "submission['id'] = id_test\n",
        "\n",
        "submission['label'] = pipe_lg_clf.predict_proba(test_lemmatized)[:,1]\n",
        "\n",
        "submission.to_csv('sample_submission_lgword_rand4.csv', index=False)"
      ],
      "metadata": {
        "id": "w_jlp9vH2Bsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This model has hyperparaneters:\n",
        "* ngram_range\": [ (1,5)]\n",
        "* max_df\":0.2\n",
        "* min_df\": 9\n",
        "* analyzer\": ['word ']\n",
        "* strip_accents\":['unicode']\n",
        "* smooth_idf':[False]\n",
        "* sublinear_tf\":[True]\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of  charand the result was using analyzer word so here **char-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.9030\n",
        " * model accuracy in kaggle public: 0.85222\n",
        " -------------------------------------------------------------------------------\n",
        " "
      ],
      "metadata": {
        "id": "ZK4s6kHucgLU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.1 XGB classifier using lemmatized data and validation set:\n",
        "in this trial the hyperparameter space:\n",
        "* ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "* max_df\": np.arange(0.2, 1.0),\n",
        "* min_df\": np.arange(5, 100),\n",
        "* strip_accents\":[None,'ascii','unicode'],\n",
        "* analyzer':['word'],\n",
        "* smooth_idf':[False,True],\n",
        "* sublinear_tf\":[True,False]\n",
        "\n",
        "this XGB classifier model cover word-level vectorizer and validation set."
      ],
      "metadata": {
        "id": "kuS3ehNYdmQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature creation and modelling in a single function\n",
        "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"xgb\", XGBClassifier(random_state=42,n_jobs=-1,eval_metric='rmse',use_label_encoder=False))])\n",
        "\n",
        "ps_2 = PredefinedSplit(split_index_lemmatized)\n",
        "\n",
        "\n",
        "# define parameter space to test\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
        "    'tfidf__analyzer':['word'],\n",
        "    'tfidf__smooth_idf':[False,True],\n",
        "    \"tfidf__sublinear_tf\":[True,False]\n",
        "}\n",
        "\n",
        "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
        "\n",
        "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=ps_2, scoring=\"roc_auc\", n_iter=45)\n",
        "pipe_xgb_clf.fit(data_lemmatized, train_data['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "bTDWRaTbmgpb",
        "outputId": "7101f3ab-5bcc-4baa-9c9a-083b79f99202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                             ('xgb',\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric='rmse',\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        'tfidf__smooth_idf': [False, True],\n",
              "                                        'tfidf__strip_accents': [None, 'ascii',\n",
              "                                                                 'unicode'],\n",
              "                                        'tfidf__sublinear_tf': [True, False]},\n",
              "                   scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;xgb&#x27;,\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric=&#x27;rmse&#x27;,\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;xgb&#x27;,\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric=&#x27;rmse&#x27;,\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;xgb&#x27;,\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=&#x27;rmse&#x27;,\n",
              "                               feature_types=None, gamma=None, gpu_id=None,\n",
              "                               grow_policy=None, importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=None,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=None, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, n_estimators=100,\n",
              "                               n_jobs=-1, num_parallel_tree=None,\n",
              "                               predictor=None, random_state=42, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;rmse&#x27;, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
              "              predictor=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
        "print('best score {}'.format(pipe_xgb_clf.best_params_))"
      ],
      "metadata": {
        "id": "ajuQQ6TJoJ_g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2169b86-bbb8-41fb-9413-3dddc9c71ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8677225239024977\n",
            "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission file for XGB model using rando search.\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = id_test\n",
        "submission['label'] = pipe_xgb_clf.predict_proba(test_lemmatized)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough_xgblemma.csv', index=False)"
      ],
      "metadata": {
        "id": "U7DjZnzqvWia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the best hyperparameter in this model:\n",
        "* ngram_range\": (1,4)\n",
        "* max_df\": 0.2\n",
        "* min_df\": 6\n",
        "* strip_accents\":['ascii']\n",
        "* analyzer':['word']\n",
        "* smooth_idf':[False]\n",
        "* sublinear_tf\":[True]\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of word and the result was using analyzer word so here **word-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.8677\n",
        " * model accuracy in kaggle public: 0.79933\n",
        " ------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "rE1hLwlmeZMY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2.2 XGB classifier using stemmed data and validation set:\n",
        "in this trial the hyperparameter space:\n",
        "* ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "* max_df\": np.arange(0.2, 1.0),\n",
        "* min_df\": np.arange(5, 100),\n",
        "* strip_accents\":[None,'ascii','unicode'],\n",
        "* analyzer':['word'],\n",
        "* smooth_idf':[False,True],\n",
        "* sublinear_tf\":[True,False]\n",
        "\n",
        "this XGB classifier model cover word-level vectorizer and validation set."
      ],
      "metadata": {
        "id": "7oq1fkWngBGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# feature creation and modelling in a single function\n",
        "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"xgb\", XGBClassifier(random_state=42,n_jobs=-1,eval_metric='rmse',use_label_encoder=False))])\n",
        "\n",
        "ps_2 = PredefinedSplit(split_index_lemmatized)\n",
        "\n",
        "\n",
        "# define parameter space to test\n",
        "params = {\n",
        "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
        "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
        "    \"tfidf__min_df\": np.arange(5, 100),\n",
        "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
        "    'tfidf__analyzer':['word'],\n",
        "    'tfidf__smooth_idf':[False,True],\n",
        "    \"tfidf__sublinear_tf\":[True,False]\n",
        "}\n",
        "\n",
        "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
        "\n",
        "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=ps_2, scoring=\"roc_auc\", n_iter=45)\n",
        "pipe_xgb_clf.fit(data_stemmed, train_data['label'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        },
        "id": "4qcv5nV126v1",
        "outputId": "d882be61-aea6-4163-c0ad-4f7af1c47b98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/xgboost/sklearn.py:1395: UserWarning: `use_label_encoder` is deprecated in 1.7.0.\n",
            "  warnings.warn(\"`use_label_encoder` is deprecated in 1.7.0.\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
              "                                             ('xgb',\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric='rmse',\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        'tfidf__smooth_idf': [False, True],\n",
              "                                        'tfidf__strip_accents': [None, 'ascii',\n",
              "                                                                 'unicode'],\n",
              "                                        'tfidf__sublinear_tf': [True, False]},\n",
              "                   scoring='roc_auc')"
            ],
            "text/html": [
              "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;xgb&#x27;,\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric=&#x27;rmse&#x27;,\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-26\" type=\"checkbox\" ><label for=\"sk-estimator-id-26\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
              "                   estimator=Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                                             (&#x27;xgb&#x27;,\n",
              "                                              XGBClassifier(base_score=None,\n",
              "                                                            booster=None,\n",
              "                                                            callbacks=None,\n",
              "                                                            colsample_bylevel=None,\n",
              "                                                            colsample_bynode=None,\n",
              "                                                            colsample_bytree=None,\n",
              "                                                            early_stopping_rounds=None,\n",
              "                                                            enable_categorical=False,\n",
              "                                                            eval_metric=&#x27;rmse&#x27;,\n",
              "                                                            feature_types=None,...\n",
              "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
              "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
              "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
              "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
              "                                        &#x27;tfidf__ngram_range&#x27;: [(1, 2), (1, 3),\n",
              "                                                               (1, 4), (1, 5)],\n",
              "                                        &#x27;tfidf__smooth_idf&#x27;: [False, True],\n",
              "                                        &#x27;tfidf__strip_accents&#x27;: [None, &#x27;ascii&#x27;,\n",
              "                                                                 &#x27;unicode&#x27;],\n",
              "                                        &#x27;tfidf__sublinear_tf&#x27;: [True, False]},\n",
              "                   scoring=&#x27;roc_auc&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-27\" type=\"checkbox\" ><label for=\"sk-estimator-id-27\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer()),\n",
              "                (&#x27;xgb&#x27;,\n",
              "                 XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "                               colsample_bylevel=None, colsample_bynode=None,\n",
              "                               colsample_bytree=None,\n",
              "                               early_stopping_rounds=None,\n",
              "                               enable_categorical=False, eval_metric=&#x27;rmse&#x27;,\n",
              "                               feature_types=None, gamma=None, gpu_id=None,\n",
              "                               grow_policy=None, importance_type=None,\n",
              "                               interaction_constraints=None, learning_rate=None,\n",
              "                               max_bin=None, max_cat_threshold=None,\n",
              "                               max_cat_to_onehot=None, max_delta_step=None,\n",
              "                               max_depth=None, max_leaves=None,\n",
              "                               min_child_weight=None, missing=nan,\n",
              "                               monotone_constraints=None, n_estimators=100,\n",
              "                               n_jobs=-1, num_parallel_tree=None,\n",
              "                               predictor=None, random_state=42, ...))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-28\" type=\"checkbox\" ><label for=\"sk-estimator-id-28\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-29\" type=\"checkbox\" ><label for=\"sk-estimator-id-29\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eval_metric=&#x27;rmse&#x27;, feature_types=None,\n",
              "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
              "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
              "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
              "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
              "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
              "              n_estimators=100, n_jobs=-1, num_parallel_tree=None,\n",
              "              predictor=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
        "print('best score {}'.format(pipe_xgb_clf.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aCTEqp1e3lXC",
        "outputId": "673c7710-8971-40a4-dbbd-8e86a926cda6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "best score 0.8430106985649337\n",
            "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 40, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a submission file for XGB model using random search.\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = id_test\n",
        "submission['label'] = pipe_xgb_clf.predict_proba(test_stemmed)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough_stemmedxgb2.csv', index=False)"
      ],
      "metadata": {
        "id": "TXznh4Dz3mN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this trial the hyperparameter space:\n",
        "* ngram_range\": [ (1,5)],\n",
        "* max_df\": 0.2\n",
        "* min_df\": 40\n",
        "* strip_accents\": None\n",
        "* analyzer':['word'],\n",
        "* smooth_idf':[False],\n",
        "* sublinear_tf\":[True]\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of word and the result was using analyzer word so here **word-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.8430\n",
        " * model accuracy in kaggle public: 0.77633\n",
        " ------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "7avOkqrxgWEX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Random Forest classifier using lemmatized data and validation set:\n",
        "Hyperparameter space:\n",
        "* n_estimators':[50,100,150,200]\n",
        "* max_depth':[3,4,5,10,15,20]\n",
        "* criterion':['gini','entropy']\n",
        "* min_samples_split':[2,3,4,6,7,8]\n",
        "\n",
        "this model cover word-level vectorizer on random forest using random search and validation set"
      ],
      "metadata": {
        "id": "gs2vGGWbhWq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the random forest pipline with random search using the param_rand parameters space.\n",
        "RF_pipline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer()),\n",
        "        ('my_classifier', \n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "RF_pipline\n",
        "\n",
        "# hyperparameter space for random forest model\n",
        "param_rand = {\n",
        "    'my_classifier__n_estimators':[50,100,150,200],\n",
        "    'my_classifier__max_depth':[3,4,5,10,15,20],\n",
        "    'my_classifier__criterion':['gini','entropy'],\n",
        "    'my_classifier__min_samples_split':[2,3,4,6,7,8],\n",
        "}\n",
        "\n",
        "rand_search_Rf = RandomizedSearchCV(RF_pipline, param_rand, cv=5, verbose=1, n_jobs=2, scoring='roc_auc')\n",
        "\n",
        "rand_search_Rf.fit(data_lemmatized, train_data['label'])\n",
        "\n",
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(rand_search_Rf.best_score_))\n",
        "print('best score {}'.format(rand_search_Rf.best_params_))"
      ],
      "metadata": {
        "id": "3fNkb0Bs3wNa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4a61e51-8532-478d-d3de-f0710d475c89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "best score 0.8384719209195897\n",
            "best score {'my_classifier__n_estimators': 150, 'my_classifier__min_samples_split': 6, 'my_classifier__max_depth': 10, 'my_classifier__criterion': 'entropy'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission file for random forest file using random forest.\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = id_test\n",
        "submission['label'] = rand_search_Rf.predict_proba(test_lemmatized)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough_lemmaRF.csv', index=False)"
      ],
      "metadata": {
        "id": "ojbvy5SLLtX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best hyperparameter combination for this model:\n",
        "* n_estimators':150\n",
        "* max_depth':10\n",
        "* criterion':['entropy']\n",
        "* min_samples_split':6\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of word and the result was using analyzer word so here **word-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.8384\n",
        " * model accuracy in kaggle public: 0.78028\n",
        " ------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "tlijpL0ZiHNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3.1 Random Forest classifier using stemmed data and validation set:\n",
        "Hyperparameter space:\n",
        "* n_estimators':[50,100,150,200]\n",
        "* max_depth':[3,4,5,10,15,20]\n",
        "* criterion':['gini','entropy']\n",
        "* min_samples_split':[2,3,4,6,7,8]\n",
        "\n",
        "this model cover word-level vectorizer on random forest using random search and validation set"
      ],
      "metadata": {
        "id": "5DMGKghti_MT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit the random forest pipline with random search using the param_rand parameters space.\n",
        "RF_pipline = Pipeline(\n",
        "    steps=[\n",
        "        (\"tfidf\", TfidfVectorizer()),\n",
        "        ('my_classifier', \n",
        "           RandomForestClassifier(),\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "RF_pipline\n",
        "\n",
        "# hyperparameter space for random forest model\n",
        "param_rand = {\n",
        "    'my_classifier__n_estimators':[50,100,150,200],\n",
        "    'my_classifier__max_depth':[3,4,5,10,15,20],\n",
        "    'my_classifier__criterion':['gini','entropy'],\n",
        "    'my_classifier__min_samples_split':[2,3,4,6,7,8],\n",
        "}\n",
        "\n",
        "rand_search_Rf = RandomizedSearchCV(RF_pipline, param_rand, cv=5, verbose=1, n_jobs=2, scoring='roc_auc')\n",
        "\n",
        "rand_search_Rf.fit(data_stemmed, train_data['label'])\n",
        "\n",
        "# see the bst score and the best hyperparameter combination.\n",
        "print('best score {}'.format(rand_search_Rf.best_score_))\n",
        "print('best score {}'.format(rand_search_Rf.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKKYjZjZK66M",
        "outputId": "9fa3c12f-0349-4433-baed-b74b189e626c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "best score 0.8155447077722325\n",
            "best score {'my_classifier__n_estimators': 150, 'my_classifier__min_samples_split': 7, 'my_classifier__max_depth': 20, 'my_classifier__criterion': 'gini'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create submission file for random forest model using random forest.\n",
        "submission = pd.DataFrame()\n",
        "submission['id'] = id_test\n",
        "submission['label'] = rand_search_Rf.predict_proba(test_stemmed)[:,1]\n",
        "submission.to_csv('sample_submission_walkthrough_stemmedRF.csv', index=False)"
      ],
      "metadata": {
        "id": "xP_D3b-cL-OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "best hyperparameter combination for this model:\n",
        "* n_estimators':150\n",
        "* max_depth':20\n",
        "* criterion':['gini']\n",
        "* min_samples_split':7\n",
        "\n",
        "in the hyperparameter space, analyzer was defined as a set of word and the result was using analyzer word so here **word-level vectorizer** is covered.\n",
        " * model accuarcy in colab: 0.8155\n",
        " * model accuracy in kaggle public: 0.80708\n",
        " ------------------------------------------------------------------------------"
      ],
      "metadata": {
        "id": "hi0PYyTRjN31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Thanks üòÉ"
      ],
      "metadata": {
        "id": "hlAM3wsYjZOz"
      }
    }
  ]
}